{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import wandb\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "from tqdm import tqdm\n",
    "import monai\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from segment_anything import sam_model_registry\n",
    "\n",
    "from model import RadSam\n",
    "from dataset import RadSamDataset\n",
    "\n",
    "join = os.path.join\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG =============================\n",
    "# Load YAML config file\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config_dict = yaml.safe_load(file)\n",
    "\n",
    "config = EasyDict(config_dict)\n",
    "run_id = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "device = config.training.device\n",
    "model_save_path = join(config.model.work_dir, config.model.task_name + \"-\" + run_id)\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "# ===================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total parameters:  93735472\n",
      "Number of trainable parameters:  4058340\n"
     ]
    }
   ],
   "source": [
    "# MODEL ==============================\n",
    "sam_model = sam_model_registry[config.training.model_type](checkpoint=config.training.checkpoint)\n",
    "radsam_model = RadSam(\n",
    "    image_encoder=sam_model.image_encoder,\n",
    "    mask_decoder=sam_model.mask_decoder,\n",
    "    prompt_encoder=sam_model.prompt_encoder,\n",
    "    freeze_img_encoder=True\n",
    ").to(device)\n",
    "\n",
    "print(\"Number of total parameters: \", sum(p.numel() for p in radsam_model.parameters())) \n",
    "print(\"Number of trainable parameters: \", sum(p.numel() for p in radsam_model.parameters() if p.requires_grad))  # 93729252\n",
    "\n",
    "img_mask_encdec_params = list(radsam_model.image_encoder.parameters()) + list(\n",
    "    radsam_model.mask_decoder.parameters()\n",
    ")\n",
    "optimizer = torch.optim.AdamW(\n",
    "    img_mask_encdec_params, lr=config.training.lr, weight_decay=config.training.weight_decay\n",
    ")\n",
    "\n",
    "# optimizer\n",
    "img_mask_encdec_params = list(radsam_model.image_encoder.parameters()) + list(\n",
    "    radsam_model.mask_decoder.parameters()\n",
    ")\n",
    "optimizer = torch.optim.AdamW(\n",
    "    img_mask_encdec_params, lr=config.training.lr, weight_decay=config.training.weight_decay\n",
    ")\n",
    "\n",
    "# loss\n",
    "seg_loss = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, reduction=\"mean\")\n",
    "ce_loss = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "# ===================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET ============================\n",
    "num_epochs = config.training.num_epochs\n",
    "iter_num = 0\n",
    "losses = []\n",
    "best_loss = 1e10\n",
    "train_dataset = RadSamDataset(\n",
    "    config.dataset.img_dir,\n",
    "    config.dataset.masks_path,\n",
    "    config.dataset.pnts_path,\n",
    "    config.dataset.start_idx,\n",
    "    config.dataset.end_idx\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.training.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.training.num_workers,\n",
    "    pin_memory=True,\n",
    ")\n",
    "# ===================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 188/3009 [02:44<41:08,  1.14it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/eustachelebihan/Development/radium-challenge/sam/train_proto.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eustachelebihan/Development/radium-challenge/sam/train_proto.ipynb#X26sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m medsam_pred \u001b[39m=\u001b[39m medsam_pred\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eustachelebihan/Development/radium-challenge/sam/train_proto.ipynb#X26sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m loss \u001b[39m=\u001b[39m seg_loss(medsam_pred, masks) \u001b[39m+\u001b[39m ce_loss(medsam_pred, masks\u001b[39m.\u001b[39mfloat())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eustachelebihan/Development/radium-challenge/sam/train_proto.ipynb#X26sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eustachelebihan/Development/radium-challenge/sam/train_proto.ipynb#X26sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eustachelebihan/Development/radium-challenge/sam/train_proto.ipynb#X26sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniforge3/envs/radium-py11/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/radium-py11/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING ===========================\n",
    "start_epoch = 0\n",
    "if config.training.resume is not None:\n",
    "    if os.path.isfile(config.training.resume):\n",
    "        ## Map model to be loaded to specified single GPU\n",
    "        checkpoint = torch.load(config.training.resume, map_location=device)\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        radsam_model.load_state_dict(checkpoint[\"model\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for step, (img_batch, points, masks, _) in enumerate(tqdm(train_dataloader)):\n",
    "        \n",
    "        img_batch = img_batch.to(device)\n",
    "        points = [points[0].to(device), points[1].to(device)]\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        medsam_pred, _ = radsam_model(img_batch, points)\n",
    "        medsam_pred = medsam_pred.squeeze().unsqueeze(0)\n",
    "        loss = seg_loss(medsam_pred, masks) + ce_loss(medsam_pred, masks.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        iter_num += 1\n",
    "    \n",
    "    epoch_loss /= step\n",
    "    losses.append(epoch_loss)\n",
    "    # ====================================\n",
    "\n",
    "    # LOGGING ============================\n",
    "    if config.training.use_wandb:\n",
    "        wandb.log({\"epoch_loss\": epoch_loss})\n",
    "    print(\n",
    "        f'Time: {datetime.now().strftime(\"%Y%m%d-%H%M\")}, Epoch: {epoch}, Loss: {epoch_loss}'\n",
    "    )\n",
    "    # save the latest model\n",
    "    checkpoint = {\n",
    "        \"model\": medsam_pred.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "    }\n",
    "    torch.save(checkpoint, join(model_save_path, \"medsam_model_latest.pth\"))\n",
    "    # save the best model\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        checkpoint = {\n",
    "            \"model\": medsam_pred.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "        }\n",
    "        torch.save(checkpoint, join(model_save_path, \"medsam_model_best.pth\"))\n",
    "    # ===================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
